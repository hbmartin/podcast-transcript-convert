1
00:00:00,000-->00:00:02,000
Hi,Brendan.Welcometothepodcast.

2
00:00:02,000-->00:00:04,000
Ritrull,pleasurehavingyouwithmetoday.

3
00:00:04,000-->00:00:07,000
YouaretheCTOandco-founderofsomethingcalled

4
00:00:07,000-->00:00:11,000
BuntingLabsandIwasintroducedtoyoubyan

5
00:00:11,000-->00:00:14,000
acquaintedtoAliceandthereasonheintroducedmetoyou

6
00:00:14,000-->00:00:17,000
wasbecauseofsomereallyinterestingworkyou'redoing

7
00:00:17,000-->00:00:21,000
aroundAIandQGISandIguesswecouldkindof

8
00:00:21,000-->00:00:24,000
describeitasanauto-completeforvectorizationbut

9
00:00:24,000-->00:00:27,000
talkmoreaboutthatlateronjustforthetimebeingperhaps

10
00:00:27,000-->00:00:28,000
youcouldfillinsomegapsforus.

11
00:00:28,000-->00:00:32,000
Ithinkofusabriefintroduction.Whoareyouandwhatdoyoudo?

12
00:00:32,000 --> 00:00:35,000
Sure. Thanks for having me on the podcast by the way.

13
00:00:35,000 --> 00:00:40,000
So I'm Brendan. I'm the CTO at Bunting Labs and that means

14
00:00:40,000 --> 00:00:45,000
that I have been building a AI auto-complete for QGIS for

15
00:00:45,000 --> 00:00:51,000
the past year. So I wasn't originally a GIS geospatial person.

16
00:00:51,000 --> 00:00:55,000
I actually got my start kind of in open source software.

17
00:00:55,000 --> 00:01:00,000
So I originally got involved with the Node.js project which is

18
00:01:00,000 --> 00:01:05,000
in open source platform for running JavaScript on servers

19
00:01:05,000 --> 00:01:09,000
normally and that's kind of where I initially

20
00:01:09,000 --> 00:01:13,000
learned to engine or software and that really gave me my

21
00:01:13,000 --> 00:01:17,000
spark in computer science and it was only later that I went to

22
00:01:17,000 --> 00:01:21,000
MIT and ended up studying physics and computer science there

23
00:01:21,000 --> 00:01:24,000
where I got more into machine learning and I think you can

24
00:01:24,000 --> 00:01:29,000
kind of see that Bunting Labs is kind of at the intersection of

25
00:01:29,000 --> 00:01:32,000
many of these different interests of mine.

26
00:01:32,000 --> 00:01:37,000
So kind of my background in machine learning and specifically

27
00:01:37,000 --> 00:01:42,000
I studied Bayesian inference at MIT kind of led me to have this

28
00:01:42,000 --> 00:01:45,000
geospatial moment towards the later end of my college career.

29
00:01:45,000 --> 00:01:50,000
And that's actually when I was working on a consulting project

30
00:01:50,000 --> 00:01:55,000
with a real estate developer and we were more or less

31
00:01:55,000 --> 00:01:59,000
summarizing different neighborhoods in terms of their characteristics

32
00:01:59,000 --> 00:02:04,000
with statistics and with neural networks and it's more or less

33
00:02:04,000 --> 00:02:07,000
at that moment where I realized that geospatial data is

34
00:02:07,000 --> 00:02:10,000
extremely powerful and I was kind of confused as to why other people

35
00:02:10,000 --> 00:02:14,000
weren't using geospatial data more and that really set me on this path

36
00:02:14,000 --> 00:02:18,000
to discover geospatial data and kind of the intersection

37
00:02:18,000 --> 00:02:22,000
between geospatial data and machine learning and that's kind of

38
00:02:22,000 --> 00:02:23,000
where Bunting Labs came from.

39
00:02:23,000 --> 00:02:27,000
I guess the big un-unanswered question for me is why

40
00:02:27,000 --> 00:02:30,000
QGIS then? Why did you decide to build something on top of

41
00:02:30,000 --> 00:02:31,000
the platform that is QGIS?

42
00:02:31,000 --> 00:02:35,000
My familiarity has always really been with open source software.

43
00:02:35,000 --> 00:02:39,000
So when I think of software that I can contribute to and

44
00:02:39,000 --> 00:02:43,000
software that I can build an ecosystem around or contribute

45
00:02:43,000 --> 00:02:46,000
to the ecosystem in I think of GitHub and I think of repositories

46
00:02:46,000 --> 00:02:50,000
that everyone can kind of see, learn from, contribute to and build

47
00:02:50,000 --> 00:02:55,000
around and so QGIS is in many ways closer to my comfort zone than

48
00:02:55,000 --> 00:02:58,000
many of the other GIS software that are available out there.

49
00:02:58,000 --> 00:03:01,000
It's extremely easy to get started with so we basically built

50
00:03:01,000 --> 00:03:06,000
the first version of this plugin in about two weeks and I could

51
00:03:06,000 --> 00:03:11,000
do that by learning from all of the other open source plugins

52
00:03:11,000 --> 00:03:15,000
that are available and so QGIS is not just this one app that

53
00:03:15,000 --> 00:03:19,000
you download to your computer, it's really an ecosystem that everyone is

54
00:03:19,000 --> 00:03:23,000
contributing software to and enjoying as a result and so I think

55
00:03:23,000 --> 00:03:26,000
that's why I first got started with the QGIS plugin.

56
00:03:26,000 --> 00:03:29,000
And so my next question is a two-part question.

57
00:03:29,000 --> 00:03:33,000
Explain to myself and into the listness of this podcast what

58
00:03:33,000 --> 00:03:38,000
auto-complete for digitizing vectors means for you and QGIS and why

59
00:03:38,000 --> 00:03:40,000
that was a problem you wanted to solve.

60
00:03:40,000 --> 00:03:47,000
So auto-complete for factorizing in QGIS or another way to kind

61
00:03:47,000 --> 00:03:52,000
of describe it is an auto-complete for tracing maps is a addition

62
00:03:52,000 --> 00:03:58,000
to the GIS pen tool and so if you are presented with raster

63
00:03:58,000 --> 00:04:03,000
imagery of some sort so if that's a satellite image of an area

64
00:04:03,000 --> 00:04:10,000
if that's a rendered PDF of some sort of construction or architecture

65
00:04:10,000 --> 00:04:14,000
asset somewhere in the world and you need to convert that into a

66
00:04:14,000 --> 00:04:19,000
vector file with a projection and some sort of like accuracy

67
00:04:19,000 --> 00:04:23,000
you would use the pen tool to digitize that into its vector

68
00:04:23,000 --> 00:04:28,000
representation and in many conversations with GIS professionals

69
00:04:28,000 --> 00:04:33,000
we found that they essentially participated in this workflow

70
00:04:33,000 --> 00:04:37,000
extremely frequently and so they would be presented with oftentimes

71
00:04:37,000 --> 00:04:40,000
a PDF that doesn't have any kind of projection with it.

72
00:04:40,000 --> 00:04:45,000
They would need to do reference that PDF to oftentimes sub beta

73
00:04:45,000 --> 00:04:49,000
resolution and that's literally locating it in aerial imagery

74
00:04:49,000 --> 00:04:52,000
or maybe on an open stream app base map and adding control points

75
00:04:52,000 --> 00:04:58,000
until the raster layer perfectly lines up with its location in the real

76
00:04:58,000 --> 00:05:01,000
world and then they would get out their pen tool and digitize

77
00:05:01,000 --> 00:05:07,000
out lines, polygons, maybe even points that represent semantic

78
00:05:07,000 --> 00:05:12,000
shapes from that map and then they use that and then add metadata

79
00:05:12,000 --> 00:05:17,000
to those shapes and so it's not just lines in your local

80
00:05:17,000 --> 00:05:22,000
state plane these are buried utility lines and so you would add

81
00:05:22,000 --> 00:05:27,000
metadata of the diameter of that pipe or maybe these are

82
00:05:27,000 --> 00:05:32,000
literally painted lines on a highway and you are adding metadata

83
00:05:32,000 --> 00:05:37,000
that says these were last painted on 2014 August 2014

84
00:05:37,000 --> 00:05:41,000
and they will need to be painted again in August 2024

85
00:05:41,000 --> 00:05:45,000
and this kind of workflow where you would take rasters and then

86
00:05:45,000 --> 00:05:50,000
you would take a meaningful digital file format equivalent of

87
00:05:50,000 --> 00:05:55,000
them we saw all the time and so given my background we started

88
00:05:55,000 --> 00:05:59,000
imagining what this workflow would look like with AI and this is

89
00:05:59,000 --> 00:06:01,000
more or less the product of that.

90
00:06:01,000 --> 00:06:05,000
You did a great job answering both questions with one answer

91
00:06:05,000 --> 00:06:09,000
that was brilliant. So given your skill set, your experience

92
00:06:09,000 --> 00:06:13,000
and your interests, if I was you I would be I would look at this

93
00:06:13,000 --> 00:06:17,000
convert every line that I can see in that rendered image to a

94
00:06:17,000 --> 00:06:20,000
you know a vector output instead of trying to like trace lines

95
00:06:20,000 --> 00:06:23,000
individually is that a thought that crossed your mind when

96
00:06:23,000 --> 00:06:25,000
you were thinking about solving this problem.

97
00:06:25,000 --> 00:06:30,000
Yeah and I think it's really funny because my conception of how to

98
00:06:30,000 --> 00:06:35,000
solve this map digitization problem is totally different now

99
00:06:35,000 --> 00:06:38,000
than it was a year ago when I first started looking at this.

100
00:06:38,000 --> 00:06:44,000
So when Michael and I decided that map digitization was a problem that we

101
00:06:44,000 --> 00:06:49,000
would tackle we basically went to the whiteboard and I threw up my design, my

102
00:06:49,000 --> 00:06:54,000
attempted solution for what digitization with AI would look like

103
00:06:54,000 --> 00:07:00,000
and I said it's probably really easy to convert an entire map into its

104
00:07:00,000 --> 00:07:05,000
vector representation. In fact there's an entire academic field dedicated to solving it.

105
00:07:05,000 --> 00:07:09,000
If you go online and Google vectorization especially of images

106
00:07:09,000 --> 00:07:13,000
there's a lot of published research as to actually accomplish that task

107
00:07:13,000 --> 00:07:19,000
and I said this is a solved problem shortly and I can build this in two weeks.

108
00:07:19,000 --> 00:07:24,000
I said that probably 13 or 14 months ago so I totally take that back

109
00:07:24,000 --> 00:07:29,000
and the first six months of me building this was actually very similar to

110
00:07:29,000 --> 00:07:34,000
your suggested solution. I did a literature search

111
00:07:34,000 --> 00:07:39,000
and found the best performing machine learning models for converting an

112
00:07:39,000 --> 00:07:44,000
image into its vector representation and there are some really high quality

113
00:07:44,000 --> 00:07:49,000
research available online for doing that. If you take pretty much any

114
00:07:49,000 --> 00:07:54,000
image we're talking especially like logos images of the real world

115
00:07:54,000 --> 00:07:59,000
and convert it into its vector format to your eye it looks fantastic.

116
00:07:59,000 --> 00:08:05,000
It is very perfectly recreating the entire image as in SVG for example.

117
00:08:05,000 --> 00:08:10,000
But when we went to kind of our original design partners and showed them this technology

118
00:08:10,000 --> 00:08:16,000
we realized that it kind of missed the mark. It's like op-re-oriented when you see a map

119
00:08:16,000 --> 00:08:21,000
it's kind of difficult to determine which of the assets in that map are

120
00:08:21,000 --> 00:08:26,000
important to have the vector representation and so we realized that if we wanted

121
00:08:26,000 --> 00:08:32,000
to productionize this we actually had to build a semantic understanding

122
00:08:32,000 --> 00:08:36,000
of all of the features in these maps and that all of a sudden was a much more

123
00:08:36,000 --> 00:08:41,000
difficult problem because that's akin to machine reading of maps and

124
00:08:41,000 --> 00:08:46,000
understanding and when we were looking at this a year ago that seemed impossible.

125
00:08:46,000 --> 00:08:51,000
And even worse when you have extremely important assets that you want a

126
00:08:51,000 --> 00:08:56,000
high quality vector representation of you don't actually trust the

127
00:08:56,000 --> 00:09:01,000
output of a machine learning model on its first go. If you were to hand me a

128
00:09:01,000 --> 00:09:05,000
geotip and I handed you back a shape file you would actually load it into your

129
00:09:05,000 --> 00:09:10,000
GIS software choice and review it such that it's perfect such that it's two

130
00:09:10,000 --> 00:09:16,000
your standards and realizing that took about six months after six or seven

131
00:09:16,000 --> 00:09:22,000
months of working on that approach I actually went on a walk with Michael and

132
00:09:22,000 --> 00:09:28,000
I essentially came to the conclusion that I was not on a trajectory of success.

133
00:09:28,000 --> 00:09:33,000
If I interpolated my progress out I would not be successful in one or two

134
00:09:33,000 --> 00:09:37,000
years and so I essentially had to go back to the drawing board and it was only

135
00:09:37,000 --> 00:09:42,000
then with inspiration kind of from these more recent generative AI models like

136
00:09:42,000 --> 00:09:48,000
Chat GBT did I consider an AI autocomplete this was never originally going to be a

137
00:09:48,000 --> 00:09:53,000
plug-in that you run in your desktop GIS software but we realized that that was

138
00:09:53,000 --> 00:09:57,000
actually the modality that people wanted they wanted to be able to have access

139
00:09:57,000 --> 00:10:01,000
to this and immediately review its output. I think to what you're getting it is

140
00:10:01,000 --> 00:10:06,000
they wanted to be in charge they wanted to have just increase or make me more

141
00:10:06,000 --> 00:10:10,000
efficient in the work that are more really doing in that way I will trust the

142
00:10:10,000 --> 00:10:15,000
output more. Could you talk to me a little bit about how this is different from

143
00:10:15,000 --> 00:10:19,000
segment anything for example because this is kind of like you give it an image

144
00:10:19,000 --> 00:10:23,000
and it segments everything in the image and people seem to love that and it seems

145
00:10:23,000 --> 00:10:27,000
to work really really well and this reminds me of your original approach like

146
00:10:27,000 --> 00:10:31,000
give me an image and I will vectorize the whole thing could you just explain

147
00:10:31,000 --> 00:10:35,000
to me in the list is please what why these two things are different and why

148
00:10:35,000 --> 00:10:41,000
I think that would be really helpful. So segment anything is extremely powerful

149
00:10:41,000 --> 00:10:46,000
but it's powerful in a few specific characteristics the first one is that

150
00:10:46,000 --> 00:10:51,000
it's grounded in text descriptions of what you're segmenting so if you take a

151
00:10:51,000 --> 00:10:56,000
very large satellite image for example I think the best use case for segment

152
00:10:56,000 --> 00:11:01,000
anything is really satellite and aerial and you can textually describe the

153
00:11:01,000 --> 00:11:06,000
geometry that you are extracting then I think segment anything and I believe

154
00:11:06,000 --> 00:11:11,000
segment anything geospatial the package that actually implements it specifically

155
00:11:11,000 --> 00:11:17,000
for geospatial software is probably the best thing that you have and so if you

156
00:11:17,000 --> 00:11:22,000
load drone imagery that you just took and ask it to segment out the lake boundary

157
00:11:22,000 --> 00:11:26,000
that's pretty much the best thing you can get I think where our AI auto

158
00:11:26,000 --> 00:11:31,000
complete is preferable to segment anything is especially when you are

159
00:11:31,000 --> 00:11:36,000
digitizing geometries that you have a semantic understanding of but are difficult

160
00:11:36,000 --> 00:11:42,000
to textually explain or you are digitizing lines that are difficult to

161
00:11:42,000 --> 00:11:48,000
semantically extract from that map and so for example we often see a lot of

162
00:11:48,000 --> 00:11:52,000
extremely low quality maybe there's a lot of JPEG artifacts on the map maybe

163
00:11:52,000 --> 00:11:57,000
the resolution is pretty low it was originally rendered to be really low or even

164
00:11:57,000 --> 00:12:02,000
those artifacts from the original scan those are kind of maps where segment

165
00:12:02,000 --> 00:12:07,000
anything struggles it's much more difficult to get segment anything to extract

166
00:12:07,000 --> 00:12:14,000
a line of a pressure sewer such that the line is a dashed line that has

167
00:12:14,000 --> 00:12:19,000
PS intermittently interrupting the line style whereas on the other hand our

168
00:12:19,000 --> 00:12:23,000
AI auto complete because it looks at basically the line that you have already

169
00:12:23,000 --> 00:12:27,000
started it excels at just completing it. Okay the line that you have already

170
00:12:27,000 --> 00:12:31,000
started in the same way auto complete works for fun using chat gbt for

171
00:12:31,000 --> 00:12:35,000
example some people describe it as a fancy auto complete where it says okay

172
00:12:35,000 --> 00:12:40,000
your question was this the next logical token is this in the same way we see

173
00:12:40,000 --> 00:12:44,000
it auto complete in Google docs for example it looks like you're the next

174
00:12:44,000 --> 00:12:49,000
logical word is here so when you say auto complete you mean I take my pen

175
00:12:49,000 --> 00:12:53,000
or I start tracing the line and it jumps ahead of me as is looks like you're

176
00:12:53,000 --> 00:12:57,000
going on this line here and follows it along am I correct. That's exactly

177
00:12:57,000 --> 00:13:03,000
right so the way our AI auto complete works is it's a drop in replacement

178
00:13:03,000 --> 00:13:09,000
for the panel and so if you activate our plug-in in qgis and begin digitizing

179
00:13:09,000 --> 00:13:14,000
some geometry let's say for our example it was a utility line on a

180
00:13:14,000 --> 00:13:19,000
raster map once you begin digitizing that line by drawing two segments a

181
00:13:19,000 --> 00:13:23,000
small bit of that map will be sent to our inference server and it will auto

182
00:13:23,000 --> 00:13:28,000
complete 50 of the next vertices of that line and these vertices are output

183
00:13:28,000 --> 00:13:32,000
by a neural network that we've specifically trained for this job and so

184
00:13:32,000 --> 00:13:37,000
that neural network is literally looking at the pixels that you have

185
00:13:37,000 --> 00:13:43,000
already drawn so as to predict which of the map is actually semantically this

186
00:13:43,000 --> 00:13:49,000
same feature and which of those pixels it should choose as continuing

187
00:13:49,000 --> 00:13:54,000
vertices and I've realized that this problem is actually much more complex than

188
00:13:54,000 --> 00:13:59,000
I originally imagined when you and I are talking about digitizing maps

189
00:13:59,000 --> 00:14:05,000
it's extremely broad and big and it can be difficult to imagine what those

190
00:14:05,000 --> 00:14:10,000
maps actually are but the magic in AI auto complete is not that it works on

191
00:14:10,000 --> 00:14:15,000
certain maps it's that it works on your map and so when someone uploads

192
00:14:15,000 --> 00:14:20,000
into qgis a map that our AI has never seen and yet is able to

193
00:14:20,000 --> 00:14:25,000
generalize based on the hundreds of maps that we've trained it on that

194
00:14:25,000 --> 00:14:28,000
I think is the really impressive thing.

195
00:14:28,000 --> 00:14:33,000
So the I think you're the example you have on your website is amazing and one

196
00:14:33,000 --> 00:14:37,000
why I think it's amazing is that you're tracing a dash line so just to

197
00:14:37,000 --> 00:14:41,000
describe this for the list is that there's a video of yeah screen video I guess

198
00:14:41,000 --> 00:14:45,000
of you tracing or someone tracing a dash line in and QJs in which is

199
00:14:45,000 --> 00:14:48,000
incredible and you can see the auto complete working in you know being

200
00:14:48,000 --> 00:14:53,000
ahead of the pen but I think the really incredible thing is that adjacent

201
00:14:53,000 --> 00:14:57,000
to that line there is an identical line crossing over that line is

202
00:14:57,000 --> 00:15:02,000
another identical line and yet the auto complete knows that I do want

203
00:15:02,000 --> 00:15:07,000
to follow this particular one of those identical lines but to me to

204
00:15:07,000 --> 00:15:10,000
the layperson this seems like an incredibly hard problem to solve.

205
00:15:10,000 --> 00:15:15,000
So I think early on I underestimated that difficulty and I'm actually

206
00:15:15,000 --> 00:15:20,000
very fortunate that my background kind of let me push through all of the

207
00:15:20,000 --> 00:15:24,000
complexities that came with solving this problem and so I think if you

208
00:15:24,000 --> 00:15:29,000
kind of follow our journey on social media it might seem as though we

209
00:15:29,000 --> 00:15:34,000
built this in a couple of weeks and all of a sudden there's AI and QJs

210
00:15:34,000 --> 00:15:38,000
but really that's a total simplification of kind of this journey I've had

211
00:15:38,000 --> 00:15:43,000
in understanding what map digitization really meant both at a

212
00:15:43,000 --> 00:15:47,000
semantic level for the people that do it professionally but also at a

213
00:15:47,000 --> 00:15:52,000
technical level and how you could teach a computer to truly learn the

214
00:15:52,000 --> 00:15:56,000
semantic features that these people are working with and can actually

215
00:15:56,000 --> 00:16:01,000
auto complete ahead of them and so to kind of dive into what is

216
00:16:01,000 --> 00:16:05,000
surprising about this I feel like I actually have a lot of you

217
00:16:05,000 --> 00:16:09,000
know as someone who's literally looked at probably a thousand different

218
00:16:09,000 --> 00:16:14,000
maps over the course of building this AI autocomplete and I've

219
00:16:14,000 --> 00:16:19,000
actually run nearly 2000 machine learning experiments to get to

220
00:16:19,000 --> 00:16:23,000
this current AI autocomplete and experiment one I promise you to

221
00:16:23,000 --> 00:16:28,000
not perform well at all and if you are actually one of our users and you

222
00:16:28,000 --> 00:16:33,000
download QJs download our plugin and try it out you are actually

223
00:16:33,000 --> 00:16:40,000
running model 1,854 and the 1,853 models that came before that were

224
00:16:40,000 --> 00:16:45,000
bad in many ways but the main important thing is that they weren't

225
00:16:45,000 --> 00:16:50,000
able to generalize into the map that you're looking at I've realized

226
00:16:50,000 --> 00:16:55,000
that lines have so many semantic meaning that allows you to

227
00:16:55,000 --> 00:17:00,000
disambiguate between two lines say when they intersect and

228
00:17:00,000 --> 00:17:03,000
encoding that into the model has been pretty much the most important

229
00:17:03,000 --> 00:17:08,000
task that I've completed. So it hits off to you for continuing I

230
00:17:08,000 --> 00:17:13,000
think if I got no what do you say 800 or what 1,800 times I would

231
00:17:13,000 --> 00:17:17,000
be tempted to give up but you didn't. And finally you got a yes

232
00:17:17,000 --> 00:17:21,000
but my guess is also that I guess you were like during that process you

233
00:17:21,000 --> 00:17:24,000
could see that you were getting better and better and better and further

234
00:17:24,000 --> 00:17:27,000
closer and closer I should say to the goal so my guess is you had

235
00:17:27,000 --> 00:17:30,000
encouragement along the way it wasn't like hard no hard no hard no right

236
00:17:30,000 --> 00:17:35,000
that many times and then yes so but anyway that is incredible

237
00:17:35,000 --> 00:17:39,000
well done well done had talked to you but when I think about this so

238
00:17:39,000 --> 00:17:43,000
digitizing lines is very impressive but it's a step in the in the

239
00:17:43,000 --> 00:17:46,000
journey right right. Georeferencing is another part of that great

240
00:17:46,000 --> 00:17:50,000
way. Now I've got my lines not to not to throw any cold water not

241
00:17:50,000 --> 00:17:53,000
to rain on your parade or anything but now I've got my lines you've

242
00:17:53,000 --> 00:17:55,000
done a great job of that I've got my lines now they need to land

243
00:17:55,000 --> 00:17:59,000
somewhere specific on the real world in order to be useful to me and

244
00:17:59,000 --> 00:18:02,000
then I need to extract some metadata about those as well is

245
00:18:02,000 --> 00:18:06,000
the any part of this the model that you built today the experience

246
00:18:06,000 --> 00:18:10,000
that you've had the learnings long the way that overlap with

247
00:18:10,000 --> 00:18:13,000
it with maybe the next step of of Georeferencing that's a great

248
00:18:13,000 --> 00:18:19,000
so I've been working simultaneously on both of these models so

249
00:18:19,000 --> 00:18:24,000
the AI autocomplete for vectorization is something that has

250
00:18:24,000 --> 00:18:30,000
kind of crested this threshold of usefulness for most of its

251
00:18:30,000 --> 00:18:34,000
time it was not useful at all and I also have a machine

252
00:18:34,000 --> 00:18:39,000
learning model on my computer that can georeference maps as

253
00:18:39,000 --> 00:18:43,000
builds documents from architecture and construction firms you

254
00:18:43,000 --> 00:18:48,000
name it automatically but it hasn't yet pressed it this threshold

255
00:18:48,000 --> 00:18:52,000
where it can actually save someone time and it's interesting

256
00:18:52,000 --> 00:18:56,000
because these two problems while they're both grounded in maps

257
00:18:56,000 --> 00:19:00,000
they're actually totally different because the AI autocomplete is

258
00:19:00,000 --> 00:19:05,000
really about understanding the semantic nature of maps and

259
00:19:05,000 --> 00:19:08,000
the features that are depicted on them whereas georeferencing is

260
00:19:08,000 --> 00:19:13,000
actually a search problem when you're presented with a map that

261
00:19:13,000 --> 00:19:16,000
you don't know where it came from especially this is extremely

262
00:19:16,000 --> 00:19:21,000
common for consultants if you are basically asked to do

263
00:19:21,000 --> 00:19:25,000
reference this map to a sub meter accuracy you often don't know

264
00:19:25,000 --> 00:19:29,000
actually where it is at all and so you embark on this interesting

265
00:19:29,000 --> 00:19:33,000
search whether you're using Google Maps you're using

266
00:19:33,000 --> 00:19:38,000
Erion satellite imagery maybe open stream map and all of these

267
00:19:38,000 --> 00:19:42,000
other kind of data sources that can help you locate where a

268
00:19:42,000 --> 00:19:46,000
particular map is and the possibilities for that are like

269
00:19:46,000 --> 00:19:51,000
astronomical you know we're talking a chess game number of

270
00:19:51,000 --> 00:19:55,000
possible game positions that's comparable to the number of

271
00:19:55,000 --> 00:20:00,000
possible ways a map could be georeference to sub meter accuracy in

272
00:20:00,000 --> 00:20:03,000
the world right so it's actually extremely difficult especially

273
00:20:03,000 --> 00:20:07,000
when the map only has a little bit of information so if the map is

274
00:20:07,000 --> 00:20:12,000
just a picture of a building and two cross streets I can guarantee

275
00:20:12,000 --> 00:20:16,000
you there are a thousand roads in the United States with those

276
00:20:16,000 --> 00:20:20,000
street names and so what people actually do is they go through

277
00:20:20,000 --> 00:20:24,000
and page through all of these possible permutations of where

278
00:20:24,000 --> 00:20:27,000
this map could be and then they line up exactly where this building is

279
00:20:27,000 --> 00:20:32,000
and so if you imagine an AI model for georeferencing it has to do

280
00:20:32,000 --> 00:20:36,000
all of that plus more and so you're synthesizing all of these

281
00:20:36,000 --> 00:20:40,000
different datasets as to you know all of the roads in the world

282
00:20:40,000 --> 00:20:46,000
not just the US obviously all of the satellite imagery or even

283
00:20:46,000 --> 00:20:50,000
high quality high resolution aerial imagery that you can use to

284
00:20:50,000 --> 00:20:55,000
georeference in exact building outline or maybe even the curb on

285
00:20:55,000 --> 00:20:59,000
a road these are challenges that you know I've had while hand

286
00:20:59,000 --> 00:21:03,000
digitizing hand to referencing maps and building in AI model to

287
00:21:03,000 --> 00:21:08,000
do that same thing is surprisingly difficult but I think

288
00:21:08,000 --> 00:21:11,000
it's something that you know we'll be able to do in hopefully

289
00:21:11,000 --> 00:21:16,000
three months it's interesting to hear you say surprisingly

290
00:21:16,000 --> 00:21:20,000
difficult because no it doesn't surprise me at all especially

291
00:21:20,000 --> 00:21:24,000
after what you just said which makes a lot of sense but the good

292
00:21:24,000 --> 00:21:27,000
uses we have a bunch of filters normally on a map you have a name

293
00:21:27,000 --> 00:21:30,000
you might have street names you might have metadata you might

294
00:21:30,000 --> 00:21:33,000
have some coordinates in there you might have something like

295
00:21:33,000 --> 00:21:37,000
this and we can filter it down to okay I know I'm looking

296
00:21:37,000 --> 00:21:41,000
in this particular town or around this address just as an example

297
00:21:41,000 --> 00:21:46,000
and I guess to not all other datasets a great your base maps

298
00:21:46,000 --> 00:21:50,000
for digitizing against so if you've got a you know PDF as

299
00:21:50,000 --> 00:21:55,000
built of some building looking at really grainy satellite data

300
00:21:55,000 --> 00:21:58,000
is not going to be super helpful it's not going to get you we

301
00:21:58,000 --> 00:22:02,000
want to go maybe open street map would be better or another

302
00:22:02,000 --> 00:22:05,000
building layer would be better where we're talking about discrete

303
00:22:05,000 --> 00:22:09,000
objects as opposed to pixels and so we can do things to filter

304
00:22:09,000 --> 00:22:13,000
that down at least that that's my sort of experience is that

305
00:22:13,000 --> 00:22:15,000
also your experience or do you can you put some other words

306
00:22:15,000 --> 00:22:18,000
around that as someone who's tried to make this model and

307
00:22:18,000 --> 00:22:22,000
is also continuously improving it there's even more complexity

308
00:22:22,000 --> 00:22:26,000
than you could imagine we've done a few projects on like

309
00:22:26,000 --> 00:22:31,000
book to referencing maps and one of the really interesting

310
00:22:31,000 --> 00:22:35,000
parts of this challenge is things change over time and so

311
00:22:35,000 --> 00:22:40,000
we've gotten basically the equivalent of a truck load of

312
00:22:40,000 --> 00:22:43,000
of PDF maps and I've been asked to do your reference these

313
00:22:43,000 --> 00:22:48,000
and the complexity of that is like highway names change

314
00:22:48,000 --> 00:22:51,000
over time right road names change over time buildings disappear

315
00:22:51,000 --> 00:22:55,000
and appear and so when you're being at rivers like change

316
00:22:55,000 --> 00:22:58,000
direction as they flow and so when you're being asked to

317
00:22:58,000 --> 00:23:02,000
locate a map the map is actually a description of how it was

318
00:23:02,000 --> 00:23:07,000
at a certain place in time and so that's another complexity

319
00:23:07,000 --> 00:23:10,000
that you're also dealing with well yeah it's a really good point

320
00:23:10,000 --> 00:23:14,000
to talk about that trust me I hadn't thought about it either

321
00:23:14,000 --> 00:23:17,000
but you also said that you're hoping to have something working in

322
00:23:17,000 --> 00:23:21,000
three months time which just seems insane but what do your

323
00:23:21,000 --> 00:23:24,000
expectations to have working three months time what what do

324
00:23:24,000 --> 00:23:27,000
you think it is that that will be working and how will it work

325
00:23:27,000 --> 00:23:30,000
I think it's really about this threshold of human usability

326
00:23:30,000 --> 00:23:34,000
for an AI autocomplete we realized that the bar was

327
00:23:34,000 --> 00:23:38,000
that the autocompleted vertices are easy to delete in case

328
00:23:38,000 --> 00:23:41,000
they're wrong but easy to continue from in case that they're

329
00:23:41,000 --> 00:23:44,000
right so we find that a lot of our users are actually okay

330
00:23:44,000 --> 00:23:49,000
with mistakes because they can instantly repair them designing

331
00:23:49,000 --> 00:23:52,000
that into our qgs plugin has actually been a great

332
00:23:52,000 --> 00:23:55,000
boon for its usability and we're kind of looking for the same

333
00:23:55,000 --> 00:23:59,000
thing with this year referencing model it's actually of

334
00:23:59,000 --> 00:24:03,000
no cost to you to try and gerruffin something acknowledge

335
00:24:03,000 --> 00:24:07,000
even if you get an error message or acknowledge that the

336
00:24:07,000 --> 00:24:10,000
gerruffin is not as accurate as you wanted it and then go in

337
00:24:10,000 --> 00:24:13,000
there and gerruffin set yourself and so you can imagine a

338
00:24:13,000 --> 00:24:16,000
gerruffin sing button in your desktop GIS software where

339
00:24:16,000 --> 00:24:20,000
you load a raster and perhaps it doesn't have a CRS perhaps

340
00:24:20,000 --> 00:24:23,000
the ground control points are wrong and it automatically

341
00:24:23,000 --> 00:24:28,000
locates its location in the real world and aligns like

342
00:24:28,000 --> 00:24:31,000
rotates that image aligns it and gives you ground control

343
00:24:31,000 --> 00:24:35,000
points to ground it in you know in its actual location

344
00:24:35,000 --> 00:24:38,000
and that experiences what we're essentially trying to

345
00:24:38,000 --> 00:24:41,000
emulate with an a i gerruffinser wow so you would

346
00:24:41,000 --> 00:24:44,000
that that's a really interesting inside that they're already

347
00:24:44,000 --> 00:24:47,000
doing that which sounds incredible already doing that

348
00:24:47,000 --> 00:24:51,000
would save a ton of time for people and be a massive

349
00:24:51,000 --> 00:24:55,000
value add for them right and because they come into this

350
00:24:55,000 --> 00:25:00,000
task with the expected cost of gerruffin sing because

351
00:25:00,000 --> 00:25:03,000
gerruffin sing something accurately can take upwards

352
00:25:03,000 --> 00:25:07,000
20 minutes if I've had maps that go longer but let's let's say

353
00:25:07,000 --> 00:25:11,000
20 minutes as a good number because you know that it can take

354
00:25:11,000 --> 00:25:14,000
that long the cost of actually waiting 10 seconds to have an

355
00:25:14,000 --> 00:25:18,000
AI model try it first that's actually pretty good and

356
00:25:18,000 --> 00:25:21,000
so that's kind of what I have in mind as I go about building

357
00:25:21,000 --> 00:25:25,000
this model now we jump back to use it to your auto

358
00:25:25,000 --> 00:25:28,000
complete for a vectorization now that it that it's working

359
00:25:28,000 --> 00:25:30,000
you've crossed that threshold of usability that you keep

360
00:25:30,000 --> 00:25:34,000
it does that mean that as the person's using it and creating

361
00:25:34,000 --> 00:25:38,000
these results are they in a way creating labels and adding

362
00:25:38,000 --> 00:25:41,000
to the model as they do it yet yes this verticie was

363
00:25:41,000 --> 00:25:44,000
right yes this verticie was no that's wrong over here

364
00:25:44,000 --> 00:25:47,000
are they improving the model over time so not by default

365
00:25:47,000 --> 00:25:51,000
so our plugin doesn't collect any unnecessary telemetry

366
00:25:51,000 --> 00:25:55,000
so we obviously know when you've requested a like

367
00:25:55,000 --> 00:25:58,000
an auto complete but we don't track whether or not you

368
00:25:58,000 --> 00:26:02,000
keep that result we don't track whether or not you delete

369
00:26:02,000 --> 00:26:06,000
the resulting completion and we don't track whether

370
00:26:06,000 --> 00:26:09,000
or not you cut that completion at a particular point all

371
00:26:09,000 --> 00:26:12,000
these are kind of potential outcomes that could happen

372
00:26:12,000 --> 00:26:16,000
in the plugin and so we don't track any telemetry as to those

373
00:26:16,000 --> 00:26:19,000
outcomes and so we don't automatically add any

374
00:26:19,000 --> 00:26:22,000
maps to our training data set we do occasionally have

375
00:26:22,000 --> 00:26:25,000
conversations with our users where they request that we

376
00:26:25,000 --> 00:26:29,000
add their map storage training to set and on occasion

377
00:26:29,000 --> 00:26:33,000
we will go and have them hand digitized we have a GIS

378
00:26:33,000 --> 00:26:36,000
staff that helps us with that but by default that's not happening

379
00:26:36,000 --> 00:26:39,000
is that in the interests of privacy or is it because

380
00:26:39,000 --> 00:26:42,000
that data would not add any you know particular value

381
00:26:42,000 --> 00:26:46,000
to to the auto complete model in the background it is

382
00:26:46,000 --> 00:26:49,000
definitely in the interest of privacy yeah let's say

383
00:26:49,000 --> 00:26:52,000
if okay so what you're saying here is that you

384
00:26:52,000 --> 00:26:55,000
would be better for the model we could make a better model if we

385
00:26:55,000 --> 00:26:57,000
collected that data about the interest of privacy

386
00:26:57,000 --> 00:27:00,000
not I realize earlier on you said that these are

387
00:27:00,000 --> 00:27:02,000
two different problems to solve the auto complete

388
00:27:02,000 --> 00:27:06,000
is a you know a semantic problem and the the georeferencing is a

389
00:27:06,000 --> 00:27:09,000
search problem assuming that you know we didn't have to think

390
00:27:09,000 --> 00:27:12,000
about privacy issues here if you also collected data for

391
00:27:12,000 --> 00:27:16,000
that the georeferencing that people were doing could you

392
00:27:16,000 --> 00:27:19,000
also improve that over time I think you could I

393
00:27:19,000 --> 00:27:23,000
think as with most ML models that are kind of deployed

394
00:27:23,000 --> 00:27:26,000
on the internet seeing how it's actually being used in the real

395
00:27:26,000 --> 00:27:30,000
world is valuable information but at least for us we found

396
00:27:30,000 --> 00:27:34,000
that we can collect really high quality data without

397
00:27:34,000 --> 00:27:39,000
having to go into our users data and I think that's a great

398
00:27:39,000 --> 00:27:42,000
great that we've kind of found ourselves in in a great

399
00:27:42,000 --> 00:27:46,000
position to be in yeah absolutely so you sound incredibly

400
00:27:46,000 --> 00:27:50,000
talented and I'm an optimist so let's say in the next few

401
00:27:50,000 --> 00:27:53,000
hours when you've solved both these problems perfectly

402
00:27:53,000 --> 00:27:56,000
and you're ready to move on to one of the next steps in the

403
00:27:56,000 --> 00:27:59,000
process which is adding metadata now that you've solved

404
00:27:59,000 --> 00:28:02,000
the the auto complete problem you've got a model that

405
00:28:02,000 --> 00:28:04,000
works 100% of the time you've got a georeferencing model that

406
00:28:04,000 --> 00:28:08,000
works 100% of the time how will you start work on collecting

407
00:28:08,000 --> 00:28:11,000
metadata what will that look like what what will be the steps

408
00:28:11,000 --> 00:28:14,000
involved in the potential output the metadata is really

409
00:28:14,000 --> 00:28:18,000
interesting because I think it brings the semantic

410
00:28:18,000 --> 00:28:22,000
understanding of the map that someone is digitizing to a

411
00:28:22,000 --> 00:28:26,000
totally new level because when you see people extracting all

412
00:28:26,000 --> 00:28:31,000
the relevant metadata for a particular project really it's

413
00:28:31,000 --> 00:28:35,000
not at the point where you're ingesting metadata it's

414
00:28:35,000 --> 00:28:38,000
not about a particular map it's how the map fits into your

415
00:28:38,000 --> 00:28:41,000
overall project and the goal that you're trying to accomplish

416
00:28:41,000 --> 00:28:45,000
and so if you are doing construction in an area and you want to

417
00:28:45,000 --> 00:28:49,000
have a complete understanding of all the buried utility lines

418
00:28:49,000 --> 00:28:53,000
such that in the in the case that you need to actually dig

419
00:28:53,000 --> 00:28:56,000
around these and what's called daylight them which is when

420
00:28:56,000 --> 00:28:59,000
you dig exactly around them such that you can see them with

421
00:28:59,000 --> 00:29:03,000
your own eyes all of that metadata that would help you

422
00:29:03,000 --> 00:29:06,000
accomplish that day lighting process is actually your goal and

423
00:29:06,000 --> 00:29:10,000
the metadata extraction is just how you get there and so I

424
00:29:10,000 --> 00:29:15,000
think this is really where the application of multi-modal

425
00:29:15,000 --> 00:29:19,000
models can really accelerate this process it's because

426
00:29:19,000 --> 00:29:24,000
when we combine the intelligence that we can generate

427
00:29:24,000 --> 00:29:28,000
about a map along with the visual and textual data

428
00:29:28,000 --> 00:29:32,000
understanding that an LLM can provide it's only with

429
00:29:32,000 --> 00:29:36,000
that larger context that metadata extraction begins to

430
00:29:36,000 --> 00:29:42,000
make sense and so if you go in to gpt4 with vision or some of

431
00:29:42,000 --> 00:29:45,000
the other multi-modal models that are available right now

432
00:29:45,000 --> 00:29:49,000
and ask it to describe a particular feature in the image

433
00:29:49,000 --> 00:29:53,000
it is totally incapable of doing that and that's really not

434
00:29:53,000 --> 00:29:57,000
surprising this data is not in these models training data sets

435
00:29:57,000 --> 00:30:01,000
and so it's not surprising that by default you can't get

436
00:30:01,000 --> 00:30:04,000
this kind of behavior from these models that being said

437
00:30:04,000 --> 00:30:07,000
it's something that I know is possible and so I'm really

438
00:30:07,000 --> 00:30:10,000
excited to be a part of that future of making it.

439
00:30:10,000 --> 00:30:13,000
Yeah I guess what I'm trying to understand is like

440
00:30:13,000 --> 00:30:15,000
what kinds of attributes would you be able to collect?

441
00:30:15,000 --> 00:30:16,000
What kinds of metadata?

442
00:30:16,000 --> 00:30:20,000
Because for me it would make sense that maybe you could

443
00:30:20,000 --> 00:30:23,000
calculate the distance of that line just as an example.

444
00:30:23,000 --> 00:30:26,000
Maybe you could look at the color of the line that you have

445
00:30:26,000 --> 00:30:29,000
just auto completed and say well I know that that matches

446
00:30:29,000 --> 00:30:33,000
to this thing over here in the legend of my map

447
00:30:33,000 --> 00:30:36,000
if that's things and then there might be some data associated

448
00:30:36,000 --> 00:30:40,000
with that it might say water pipe, sewage pipe, I don't know

449
00:30:40,000 --> 00:30:43,000
gas pipe, that kind of thing that would be valuable data to have in there.

450
00:30:43,000 --> 00:30:45,000
Maybe I've digitized other lines maybe it could say

451
00:30:45,000 --> 00:30:48,000
well this pipe is so close to the other pipe

452
00:30:48,000 --> 00:30:53,000
maybe you could start building up this database based on the metadata

453
00:30:53,000 --> 00:30:56,000
that you're collecting about each object based on its physical characteristics

454
00:30:56,000 --> 00:30:58,000
based on whatever else is in the map.

455
00:30:58,000 --> 00:31:01,000
That is the way I would think of it but I'm sure you have other ideas.

456
00:31:01,000 --> 00:31:06,000
So in terms of metadata that a user could probably extract

457
00:31:06,000 --> 00:31:12,000
from these maps automatically a really simple example is data on a legend

458
00:31:12,000 --> 00:31:15,000
but that's not really the exciting applications.

459
00:31:15,000 --> 00:31:17,000
Data on a legend is like pretty easy to extract

460
00:31:17,000 --> 00:31:22,000
and so it's not really the level where you're saving a GIS analyst

461
00:31:22,000 --> 00:31:24,000
or a GIS technician a lot of time.

462
00:31:24,000 --> 00:31:27,000
It's really about metadata that's hidden in these maps

463
00:31:27,000 --> 00:31:31,000
and so I'm going to continue with this subsurface utility example

464
00:31:31,000 --> 00:31:33,000
even though it's kind of misleading.

465
00:31:33,000 --> 00:31:38,000
If you were to daylight barred utility lines you would do a more

466
00:31:38,000 --> 00:31:41,000
it would be a more complicated process to actually do that

467
00:31:41,000 --> 00:31:45,000
and you wouldn't do that with just QLD raster images, raster maps

468
00:31:45,000 --> 00:31:48,000
but hypothetically really important metadata to know

469
00:31:48,000 --> 00:31:53,000
if you were daylighting a particular barred utility line is actually the depth of that line

470
00:31:53,000 --> 00:31:58,000
and also the trait of the soil that it's buried in.

471
00:31:58,000 --> 00:32:02,000
These are kind of important things for the construction crews to know

472
00:32:02,000 --> 00:32:06,000
if they were to be delicately digging around a gas line

473
00:32:06,000 --> 00:32:09,000
such that it wouldn't have a dangerous utility strike

474
00:32:09,000 --> 00:32:13,000
and so that's kind of a good example in the utility space.

475
00:32:13,000 --> 00:32:18,000
We also have our users outside of architecture engineering construction.

476
00:32:18,000 --> 00:32:20,000
Another good example is actually mining and geology

477
00:32:20,000 --> 00:32:27,000
and so if you are a geologist and you are dealing with kind of a older geologic map

478
00:32:27,000 --> 00:32:31,000
that describes the deposits in an area.

479
00:32:31,000 --> 00:32:37,000
The metadata associated with those deposits can be pretty complex

480
00:32:37,000 --> 00:32:41,000
especially when the legend associated with that.

481
00:32:41,000 --> 00:32:45,000
It's not even about describing a solid color region of that map

482
00:32:45,000 --> 00:32:52,000
but rather the matching the exact stylistic look of that polygon

483
00:32:52,000 --> 00:32:55,000
to a tiny little bit in the region.

484
00:32:55,000 --> 00:33:00,000
That kind of says this is a sediment deposit of some type

485
00:33:00,000 --> 00:33:04,000
and so it's metadata like that that we would want to extract automatically

486
00:33:04,000 --> 00:33:09,000
and actually embed into shape files or geopackages such that these projects can

487
00:33:09,000 --> 00:33:11,000
be accomplished more quickly.

488
00:33:11,000 --> 00:33:16,000
Well, it looks like you have got your hands full for the next little while.

489
00:33:16,000 --> 00:33:22,000
What more generally are you surprised at the lack of AI being used in QDS for example

490
00:33:22,000 --> 00:33:26,000
or GIS tools and especially when we think of desktop tools?

491
00:33:26,000 --> 00:33:30,000
For me, this is one of the few examples that I've seen out there in the world

492
00:33:30,000 --> 00:33:36,000
where it's being embedded into the actual tool itself, into the GIS tool itself

493
00:33:36,000 --> 00:33:39,000
but maybe you've seen more, maybe you've seen less.

494
00:33:39,000 --> 00:33:42,000
I don't know but I'd really like to hear your thoughts about that.

495
00:33:42,000 --> 00:33:47,000
As an ML oriented person, I think there's a big gap between what is possible

496
00:33:47,000 --> 00:33:51,000
in desktop GIS software and what is already happening.

497
00:33:51,000 --> 00:33:56,000
So when people see large language models like ChatGPT,

498
00:33:56,000 --> 00:34:01,000
GPD for a lot of the other major developments that have been happening

499
00:34:01,000 --> 00:34:04,000
especially in terms of video generation, audio processing,

500
00:34:04,000 --> 00:34:08,000
it's clear that pretty much all complex problems

501
00:34:08,000 --> 00:34:14,000
will eventually be solved by artificial intelligence until it relates to scarcity.

502
00:34:14,000 --> 00:34:19,000
I think unless you're talking about material scarcity that is like literally the amount of food

503
00:34:19,000 --> 00:34:25,000
that's available on Earth, a lot of these problems can and will be solved by AI and ML.

504
00:34:25,000 --> 00:34:32,000
And I think it's only because we're at the beginning of this renaissance in machine learning

505
00:34:32,000 --> 00:34:37,000
that we see attention to it but everyone's talking about it and nobody is implementing it.

506
00:34:37,000 --> 00:34:42,000
I think that's the best way to kind of imagine how this renaissance is happening right now.

507
00:34:42,000 --> 00:34:49,000
Obviously open AI and these larger AI research companies are driving most of the innovation

508
00:34:49,000 --> 00:34:51,000
in terms of where AI is going.

509
00:34:51,000 --> 00:34:55,000
Meta being actually an extremely great example who created segment anything

510
00:34:55,000 --> 00:34:59,000
and is releasing a lot of this groundbreaking research publicly.

511
00:34:59,000 --> 00:35:04,000
But I think most of the opportunity that exists in machine learning today

512
00:35:04,000 --> 00:35:11,000
is actually in building domain specific models and embedding them into professionals workflows.

513
00:35:11,000 --> 00:35:15,000
So I think GIS is a really great example of this.

514
00:35:15,000 --> 00:35:23,000
You can't take an LLM even if it's memorized the entirety of the QGIS documentation

515
00:35:23,000 --> 00:35:26,000
plus all mentions of GIS online ever.

516
00:35:26,000 --> 00:35:32,000
That LLM will often not immediately accelerate a professionals workflow.

517
00:35:32,000 --> 00:35:37,000
But once you understand that these large models are doing much more than generate text,

518
00:35:37,000 --> 00:35:43,000
they're actually generating semantic understanding of what you are asking it to do.

519
00:35:43,000 --> 00:35:46,000
And then completing that task as a result.

520
00:35:46,000 --> 00:35:50,000
Once you see that semantic understanding is actually what's being accomplished here,

521
00:35:50,000 --> 00:35:55,000
you can embed semantic understanding into all programs.

522
00:35:55,000 --> 00:35:59,000
And that program can understand your overall task just as much as you do.

523
00:35:59,000 --> 00:36:04,000
And I think we will see that trend more generally within GIS, within CAD,

524
00:36:04,000 --> 00:36:07,000
within all of these kind of desktop softwares.

525
00:36:07,000 --> 00:36:09,000
And it's not just about text.complete.

526
00:36:09,000 --> 00:36:11,000
It's really about augmentation.

527
00:36:11,000 --> 00:36:16,000
If you weren't working on this problem of auto-complete with digitization of geo-referencing,

528
00:36:16,000 --> 00:36:18,000
automatic extraction of metadata,

529
00:36:18,000 --> 00:36:24,000
and thinking again about working within and on top of the QGIS platform

530
00:36:24,000 --> 00:36:27,000
and solving problems for geospatial professionals

531
00:36:27,000 --> 00:36:31,000
that were using this as the main tool, what problem would you work on?

532
00:36:31,000 --> 00:36:33,000
That's a great question.

533
00:36:33,000 --> 00:36:39,000
So pretty much all of my work so far has been around mapping the built world,

534
00:36:39,000 --> 00:36:41,000
or even the natural world.

535
00:36:41,000 --> 00:36:47,000
But an interesting opportunity for the geospatial world to move towards

536
00:36:47,000 --> 00:36:51,000
is less about mapping its current state,

537
00:36:51,000 --> 00:36:54,000
but more about imagining what's possible.

538
00:36:54,000 --> 00:36:58,000
And there's a couple of examples of startups actually doing this,

539
00:36:58,000 --> 00:37:03,000
where you take your conception of how the world is now

540
00:37:03,000 --> 00:37:08,000
and pretty much synthesize a way that the world could look in the future

541
00:37:08,000 --> 00:37:11,000
and evaluate whether or not that version is better.

542
00:37:11,000 --> 00:37:15,000
There are some really great advancements in how people are doing this

543
00:37:15,000 --> 00:37:19,000
to, for example, make the world a greener place,

544
00:37:19,000 --> 00:37:27,000
because geospatial software can literally find the optimal place to put a wind farm

545
00:37:27,000 --> 00:37:29,000
or a solar farm.

546
00:37:29,000 --> 00:37:32,000
Once you consider all the complexity that's associated with that.

547
00:37:32,000 --> 00:37:36,000
But I think it goes much deeper than just real estate development.

548
00:37:36,000 --> 00:37:40,000
It's actually about evaluating ways on a map

549
00:37:40,000 --> 00:37:44,000
that the world is a bad place and changing that in the future.

550
00:37:44,000 --> 00:37:47,000
And that's really magical.

551
00:37:47,000 --> 00:37:53,000
A fun example of that is actually when you map out the street trees in a neighborhood.

552
00:37:53,000 --> 00:37:56,000
This is especially true in the United States,

553
00:37:56,000 --> 00:38:00,000
where you have kind of large roads and not much shade.

554
00:38:00,000 --> 00:38:04,000
They don't really encourage pedestrian-friendly neighborhoods.

555
00:38:04,000 --> 00:38:08,000
But you can actually use machine learning and statistics on geospatial data

556
00:38:08,000 --> 00:38:12,000
to find the best spot to plant large trees

557
00:38:12,000 --> 00:38:16,000
and create more shade, create a better spot to walk around.

558
00:38:16,000 --> 00:38:19,000
And basically improve the nearby residents' lives.

559
00:38:19,000 --> 00:38:25,000
So I think if I wasn't building AI autocomplete for this geomorphencing,

560
00:38:25,000 --> 00:38:28,000
vectorizing and metadata extraction workflow,

561
00:38:28,000 --> 00:38:30,000
I would be working on changing what's actually in the map.

562
00:38:30,000 --> 00:38:31,000
That was a great answer.

563
00:38:31,000 --> 00:38:35,000
I think I'm going to need a few minutes to walk around the house after this interview

564
00:38:35,000 --> 00:38:37,000
and think about that.

565
00:38:37,000 --> 00:38:39,000
I appreciate it. You've really given me food for thought.

566
00:38:39,000 --> 00:38:42,000
This is also probably a great time to wrap up the conversation.

567
00:38:42,000 --> 00:38:44,000
And thank you very much for your time.

568
00:38:44,000 --> 00:38:47,000
It's much appreciated. And thank you very much for your work.

569
00:38:47,000 --> 00:38:49,000
I think it's fascinating. I think it's really interesting.

570
00:38:49,000 --> 00:38:53,000
And above all, I think it's going to be incredibly helpful to a lot of people.

571
00:38:53,000 --> 00:38:55,000
So if people want to check out what you're doing,

572
00:38:55,000 --> 00:38:57,000
where is the best place to they can go?

573
00:38:57,000 --> 00:38:59,000
Can they reach out to you? Is there a website?

574
00:38:59,000 --> 00:39:01,000
What, where can we send them?

575
00:39:01,000 --> 00:39:03,000
Yeah, thank you, Daniel.

576
00:39:03,000 --> 00:39:06,000
If you're interested in seeing what we're up to,

577
00:39:06,000 --> 00:39:10,000
you can go to our website, which is bunting labs, B-U-N-T,

578
00:39:10,000 --> 00:39:16,000
I-N-G, labs.com. And you can also follow us on Twitter at the same handle.

579
00:39:16,000 --> 00:39:19,000
Yeah, thank you so much for the opportunity to share what we're up to.

580
00:39:19,000 --> 00:39:21,000
No problem. Any time. I appreciate your work.

581
00:39:21,000 --> 00:39:23,000
And I wish you all the best in the future.

582
00:39:23,000 --> 00:39:24,000
Cheers.

583
00:39:24,000 --> 00:39:25,000
Thanks.